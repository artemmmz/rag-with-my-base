{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-25T11:26:54.559896Z",
     "start_time": "2025-05-25T11:26:46.167058Z"
    }
   },
   "source": [
    "from core.vector_store import vector_store\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_g4f import G4FLLM\n",
    "\n",
    "from g4f import Provider"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Artem/PycharmProjects/rag-my-base/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T11:27:01.514056Z",
     "start_time": "2025-05-25T11:27:01.508007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever_model = G4FLLM(\n",
    "    provider=Provider.Qwen_Qwen_2_5,\n",
    "    model=Provider.Qwen_Qwen_2_5.default_model,\n",
    "    create_kwargs={\"temperature\": 0.4}\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "retriever_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are a multilingual query rewriter helping improve document retrieval from a vector database.\n",
    "    Your task is to generate several diverse reformulations of the input query to maximize recall.\n",
    "    Include at least 2 queries in Russian and 2 in English. Rephrase the query using:\n",
    "\n",
    "    - Synonyms\n",
    "    - Common phrases\n",
    "    - Abbreviations and their expansions\n",
    "    - Related technical or domain-specific terms\n",
    "    - Alternative grammatical structures\n",
    "\n",
    "    Original user question: {question}\n",
    "\n",
    "    Return 6–8 diverse, semantically related queries in a list format. Avoid exact duplicates. Ensure both Russian and English are represented.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=retriever_model,\n",
    "    prompt=retriever_prompt\n",
    ")"
   ],
   "id": "e423f60fa6205196",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T09:14:36.209693Z",
     "start_time": "2025-05-19T09:14:36.207234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ],
   "id": "e4280a137176c9d1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T09:14:38.241674Z",
     "start_time": "2025-05-19T09:14:38.211718Z"
    }
   },
   "cell_type": "code",
   "source": "retriever.invoke(\"BERT\")",
   "id": "71662e9189a90db6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='71005a36-3e1f-470e-854f-5a23c9196ca1', metadata={'arxiv_url': 'https://arxiv.org/abs/2304.054681', 'id': '2304.05468v1', 'summary': 'The Serbian language is a Slavic language spoken by over 12 million speakers and well understood by over 15 million people. In the area of natural language processing, it can be considered a low-resourced language. Also, Serbian is considered a high-inflectional language. The combination of many word inflections and low availability of language resources makes natural language processing of Serbian challenging. Nevertheless, over the past three decades, there have been a number of initiatives to develop resources and methods for natural language processing of Serbian, ranging from developing a corpus of free text from books and the internet, annotated corpora for classification and named entity recognition tasks to various methods and models performing these tasks. In this paper, we review the initiatives, resources, methods, and their availability.', 'title': 'A Survey of Resources and Methods for Natural Language Processing of   Serbian Language'}, page_content='BERT.'),\n",
       " Document(id='12bda569-b399-4a7c-958c-2bc2bbb5badf', metadata={'arxiv_url': 'https://arxiv.org/abs/1906.068212', 'id': '1906.06821v2', 'summary': 'Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.', 'title': 'A Survey of Optimization Methods from a Machine Learning Perspective'}, page_content='\\\\par'),\n",
       " Document(id='1de727a7-bc6c-4955-9164-5cba89612581', metadata={'arxiv_url': 'https://arxiv.org/abs/1906.068212', 'id': '1906.06821v2', 'summary': 'Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.', 'title': 'A Survey of Optimization Methods from a Machine Learning Perspective'}, page_content='\\\\par'),\n",
       " Document(id='88da6f08-3a3c-43d3-bf23-d39d3148dfa8', metadata={'arxiv_url': 'https://arxiv.org/abs/1906.068212', 'id': '1906.06821v2', 'summary': 'Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.', 'title': 'A Survey of Optimization Methods from a Machine Learning Perspective'}, page_content='\\\\par')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T09:14:44.017188Z",
     "start_time": "2025-05-19T09:14:40.579674Z"
    }
   },
   "cell_type": "code",
   "source": "retriever_from_llm.invoke(\"Обратные матрицы\")",
   "id": "41a4f863426196aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Матрицы обратные', '2. Как найти обратную матрицу?', '3. Обратные матрицы в линейной алгебре', '4. Inverse matrices', '5. How to calculate the inverse of a matrix?', '6. Finding the reciprocal of a matrix', '7. Обратная матрица: методы и примеры', '8. What is an inverse matrix in linear algebra?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='3a8c597a-1bf7-46b3-b36f-7b6f4c2dfd3a', metadata={'title': 'Отображения', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D1%82%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F'}, page_content='Инъективноеотображение — переводит разные элементы A в разные элементы B: Сюръективноеотображение(на множестве B) — каждый элемент множества B является образом хотя бы одного элемента множества A: Биективноеотображение — инъекция + сюръекция — взаимно однозначное соответствие, обладает двумя предыдущими свойствами. - Множества'),\n",
       " Document(id='062a14c4-a970-49a3-aa47-0052ecabcf70', metadata={'title': 'Исчисление предикатов', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%98%D1%81%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D0%B2'}, page_content='множество[math]D[/math]- это будет множество всех существ,[math]S(x)[/math]- предикат \"быть смертным\",[math]H(x)[/math]- предикат \"быть человеком\". Тогда фраза в полу-формальном виде выглядит так: Для каждого[math]x[/math], такого, что[math]H(x)[/math]верно[math]S(x)[/math], поэтому поскольку[math]H[/math](Сократ), значит, что имеет место[math]S[/math](Сократ). Чтобы построить новое исчисление, нам требуется указать 3 компонента: язык, аксиомы и правила вывода. Добавим к языку исчисления высказываний новые конструкции с предикатами и получим язык исчисления предикатов. Вот расширенная грамматика: - <выражение> ::= <импликация> - <импликация> ::= <дизъюнкция> | <дизъюнкция>[math] \\\\rightarrow'),\n",
       " Document(id='65153832-803e-47bf-84f2-fbd22972c47f', metadata={'title': 'Исчисление предикатов', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%98%D0%BD%D1%82%D0%B5%D1%80%D0%BF%D1%80%D0%B5%D1%82%D0%B0%D1%86%D0%B8%D1%8F_%D0%B1%D1%83%D0%BB%D0%B5%D0%B2%D1%8B%D1%85_%D1%84%D0%BE%D1%80%D0%BC%D1%83%D0%BB_%D1%81_%D0%BA%D0%B2%D0%B0%D0%BD%D1%82%D0%BE%D1%80%D0%B0%D0%BC%D0%B8_%D0%BA%D0%B0%D0%BA_%D0%B8%D0%B3%D1%80_%D0%B4%D0%BB%D1%8F_%D0%B4%D0%B2%D1%83%D1%85_%D0%B8%D0%B3%D1%80%D0%BE%D0%BA%D0%BE%D0%B2'}, page_content='множество[math]D[/math]- это будет множество всех существ,[math]S(x)[/math]- предикат \"быть смертным\",[math]H(x)[/math]- предикат \"быть человеком\". Тогда фраза в полу-формальном виде выглядит так: Для каждого[math]x[/math], такого, что[math]H(x)[/math]верно[math]S(x)[/math], поэтому поскольку[math]H[/math](Сократ), значит, что имеет место[math]S[/math](Сократ). Чтобы построить новое исчисление, нам требуется указать 3 компонента: язык, аксиомы и правила вывода. Добавим к языку исчисления высказываний новые конструкции с предикатами и получим язык исчисления предикатов. Вот расширенная грамматика: - <выражение> ::= <импликация> - <импликация> ::= <дизъюнкция> | <дизъюнкция>[math] \\\\rightarrow'),\n",
       " Document(id='954578cc-f633-406a-afbb-803e886800cc', metadata={'title': 'Исчисление предикатов', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D1%8F_4'}, page_content='множество[math]D[/math]- это будет множество всех существ,[math]S(x)[/math]- предикат \"быть смертным\",[math]H(x)[/math]- предикат \"быть человеком\". Тогда фраза в полу-формальном виде выглядит так: Для каждого[math]x[/math], такого, что[math]H(x)[/math]верно[math]S(x)[/math], поэтому поскольку[math]H[/math](Сократ), значит, что имеет место[math]S[/math](Сократ). Чтобы построить новое исчисление, нам требуется указать 3 компонента: язык, аксиомы и правила вывода. Добавим к языку исчисления высказываний новые конструкции с предикатами и получим язык исчисления предикатов. Вот расширенная грамматика: - <выражение> ::= <импликация> - <импликация> ::= <дизъюнкция> | <дизъюнкция>[math] \\\\rightarrow'),\n",
       " Document(id='1af0bf97-0a23-4b1e-8f15-aec968865664', metadata={'title': 'Задача о порядке перемножения матриц', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D0%BE_%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BA%D0%B5_%D0%BF%D0%B5%D1%80%D0%B5%D0%BC%D0%BD%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86'}, page_content='мы видим, первый способ гораздо эффективней. В данной задаче нужно узнать минимальное количество операций (или минимальную стоимость), необходимых для перемножения матриц. Если перемножить только две матрицы, то можно осуществить это едиственным способом, следовательно минимальная стоимость — это стоимость перемножения этих двух матриц. В общем, можно найти минимальную стоимость используя следующийрекурсивный алгоритм: - взять последовательность матриц и разделить её на две части, - найти минимальную стоимость перемножения на каждой подпоследовательности, - сложить эти две стоимости и прибавить к этому стоимость перемножения двух получившихся матриц, - сделать это для каждой возможной'),\n",
       " Document(id='4ab0c242-aec4-45af-bca3-bf03e8aedcb1', metadata={'title': 'Обратная матрица', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0'}, page_content='исходной матрицы; - [math]{a}_{ij}[/math]— элементы исходной матрицы. Алгебраическим дополнениемэлемента[math]\\\\ a_{ij}[/math]матрицы[math]\\\\ A[/math]называется число [math]\\\\ A_{ij}=(-1)^{i+j}M_{ij}[/math], где[math]\\\\ M_{ij}[/math]— дополнительный минор, определитель матрицы, получающейся из исходной матрицы[math]\\\\ A[/math]путем вычёркиванияi-й строки иj-го столбца. [math]M_{ij} = det\\\\begin{pmatrix} a_{11} & a_{12} & \\\\cdots & a_{1(j-1)} & a_{1(j+1)} & \\\\cdots & a_{1n} \\\\\\\\ \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ a_{(i-1)1} & a_{(i-1)2} & \\\\cdots & a_{(i-1)(j-1)} & a_{(i-1)(j+1)} & \\\\cdots & a_{(i-1)n} \\\\\\\\ a_{(i+1)1} & a_{(i+1)2} & \\\\cdots & a_{(i+1)(j-1)} & a_{(i+1)(j+1)} &'),\n",
       " Document(id='184f0171-9ce7-4e31-9d8d-2e146a149f0e', metadata={'title': 'Группа', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%93%D1%80%D1%83%D0%BF%D0%BF%D0%B0'}, page_content='перемножаются и их определители, матричное умножение не выводит из множества матриц с единичным определителем, и это множество образует группу (учитывая существование единичных и обратных матриц). Нейтральный элемент — единичная матрица, обратный — обратная матрица. Подстановка — взаимно однозначное отображение конечного множества на себя. Если некоторая совокупность подстановок замкнута относительно композиции отображений, определяющей бинарную операцию для подстановок на одном и том же множестве, то эта совокупность называетсягруппой подстановок. Задания на группы'),\n",
       " Document(id='61097a89-e6b4-40f1-8494-006d1e2623b3', metadata={'title': 'Обратная матрица', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0'}, page_content='матрицу к единичной, применяя все операции одновременно и к левой, и к правой матрицам. - 4)[math]A^{-1} = B[/math] [math]\\\\widehat{A}= \\\\begin{pmatrix} {A}_{11} & {A}_{12} & \\\\cdots & {A}_{1n} \\\\\\\\ {A}_{21} & {A}_{22} & \\\\cdots & {A}_{2n} \\\\\\\\ \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ {A}_{n1} & {A}_{n2} & \\\\cdots & {A}_{nn} \\\\\\\\ \\\\end{pmatrix}[/math] Исходная матрица: [math]{A}= \\\\begin{pmatrix} {a}_{11} & {a}_{12} & \\\\cdots & {a}_{1n} \\\\\\\\ {a}_{21} & {a}_{22} & \\\\cdots & {a}_{2n} \\\\\\\\ \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ {a}_{n1} & {a}_{n2} & \\\\cdots & {a}_{nn} \\\\\\\\ \\\\end{pmatrix}[/math] Где: - [math]\\\\widehat{A}[/math]— присоединённая(союзная, взаимная) матрица; - [math]{A}_{ij}[/math]— алгебраические дополнения'),\n",
       " Document(id='6bdf249c-8661-455a-abfb-8cb8f64ba298', metadata={'title': 'Матричное представление перестановок', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D1%80%D0%B8%D1%87%D0%BD%D0%BE%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BF%D0%B5%D1%80%D0%B5%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BE%D0%BA'}, page_content='перестановкой второго и третьего столбцов. Благодаря своим свойствам, матрицам перестановок нашлось применение в линейной алгебре. Они используются в элементарных преобразованиях матриц, то есть домножение слева или справа на матрицу перестановок, есть перестановка любых строк или столбов соответственно. Пример Пусть задана матрица перестановки[math]P = \\\\begin{pmatrix} 1 & 0 & 0 \\\\\\\\ 0 & 0 & 1 \\\\\\\\ 0 & 1 & 0 \\\\\\\\ \\\\end{pmatrix}[/math], которая соответствует перестановке[math]\\\\pi = \\\\begin{pmatrix} 1 & 2 & 3 \\\\\\\\ 1 & 3 & 2 \\\\end{pmatrix}[/math], и матрица[math]A = \\\\begin{pmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\\\\\\\ 7 & 8 & 9 \\\\\\\\ \\\\end{pmatrix}[/math], тогда перемножив получим: - [math]PA = \\\\begin{pmatrix} 1 & 0 &'),\n",
       " Document(id='0b71b5df-500f-4eb3-aefd-fb4747dd5e63', metadata={'title': 'Матричное представление перестановок', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D1%80%D0%B8%D1%87%D0%BD%D0%BE%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BF%D0%B5%D1%80%D0%B5%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BE%D0%BA'}, page_content='Каждая матрица перестановки размера[math]n \\\\times n[/math]является матричным представлением перестановки порядка[math]n[/math]. Пусть дана перестановка[math]\\\\sigma[/math]порядка[math]n[/math]: Соответствующей матрицей перестановки является матрица[math]n \\\\times n[/math]вида: Пусть дана перестановка:[math]\\\\pi = \\\\begin{pmatrix} 1 & 2 & 3\\\\\\\\ 1 & 3 & 2 \\\\end{pmatrix}[/math]. В соответствующей матрице в первом столбце единица будет стоять на первом месте, во втором столбе на третьем месте, в третьем на втором. Итого:[math]P = \\\\begin{pmatrix} 1 & 0 & 0 \\\\\\\\ 0 & 0 & 1 \\\\\\\\ 0 & 1 & 0 \\\\\\\\ \\\\end{pmatrix}[/math]. Также эта матрица является элементарной матрицей перестановок, так как получена из единичной,'),\n",
       " Document(id='2a4292ed-5ca2-4e4b-b579-8d55a73a0c7c', metadata={'title': 'Локальная теорема о неявном отображении', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%9B%D0%BE%D0%BA%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F_%D1%82%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%BE_%D0%BD%D0%B5%D1%8F%D0%B2%D0%BD%D0%BE%D0%BC_%D0%BE%D1%82%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B8'}, page_content=\"матрицей будет матрица Якоби[math]g'_{\\\\overline y}(\\\\overline x,\\\\overline y)[/math]. Раз она обратима в[math](x_0,y_0)[/math], то по непрерывности она будет обратима в окрестности этой точки, следовательно,[math]dy[/math]можно выразить через[math]dx[/math], формулы будут линейны. [math]dy_1=\\\\sum\\\\limits_{j=1}^n A_{1j}dx_j[/math]. Тогда, подставляя эти форулы в[math](*)[/math], получим[math]\\\\sum\\\\limits_{j=1}^n B_j dx_j=0 \\\\Rightarrow \\\\forall j : B_j = 0[/math], так как дифференцируются независимые переменные. Мы получили систему уравнений для полученных точек, похожих на условный экстремум; которую надо решать вместе с уравнениями связи. На самом деле, этому можно придать более удобную форму,\"),\n",
       " Document(id='8c765916-0215-4bf9-9403-b273f8248295', metadata={'title': 'Сверточные нейронные сети', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%A1%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D0%B5_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8'}, page_content='0}^{m_x-1}\\\\sum_{v = 0}^{m_y - 1}A_{i+u,j+v}B_{u,v}[/math]. НаРисунке 1можно видеть, как матрица[math]B[/math]«двигается» по матрице[math]A[/math], и в каждом положении считается скалярное произведение матрицы[math]B[/math]и той части матрицы[math]A[/math], на которую она сейчас наложена. Получившееся число записывается в соответствующий элемент результата. Логический смысл свертки такой — чем больше величина элемента свертки, тем больше эта часть матрицы[math]A[/math]была похожа на матрицу[math]B[/math](похожа в смысле скалярного произведения). Поэтому матрицу[math]A[/math]называютизображением, а матрицу[math]B[/math]—фильтромилиобразцом. В сверточной нейронной сети выходы промежуточных'),\n",
       " Document(id='5d93d96c-fe25-4aca-99d1-a07fea19b926', metadata={'arxiv_url': 'https://arxiv.org/abs/1906.068212', 'id': '1906.06821v2', 'summary': 'Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.', 'title': 'A Survey of Optimization Methods from a Machine Learning Perspective'}, page_content='where $A$ is an $n\\\\times n$ symmetric, positive-definite matrix. The matrix $A$ and vector $b$ are known, and we need to solve the value of $\\\\theta$. The problem (\\\\ref{3.1.1}) can also be considered as an optimization problem that minimizes the quadratic positive definite function,\\n\\\\begin{equation}\\n\\\\operatorname*{min}\\\\limits_{\\\\theta}\\\\ \\\\ F(\\\\theta)=\\\\frac{1}{2}\\\\theta^{\\\\top}  A\\\\theta-b\\\\theta+c.\\n\\\\end{equation} \\nThe above two equations have an identical unique solution. It enables us to regard the conjugate gradient as a method for solving optimization problems.\\n\\\\par'),\n",
       " Document(id='a064dbb6-6af4-4bdd-ae56-5b29d0096452', metadata={'title': 'Задача о расстановке знаков в выражении', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D0%BE_%D1%80%D0%B0%D1%81%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B5_%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D0%B2_%D0%B2_%D0%B2%D1%8B%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B8'}, page_content='Данная задача решается с использованиемпринципа оптимальности на подотрезках. Введём матрицу[math]d[/math]размером[math]n \\\\times n[/math], где[math]d[i][j][/math]будет равен максимальному значению, достигаемому на подотрезке[math]a_i, a_{i+1}, \\\\ldots, a_j[/math]. Получаем следующие соотношения: - [math]d[i][i] = a_i [/math] - [math]d[i][j] = \\\\max\\\\limits_{\\\\mathop{k = i..j-1}}[\\\\max(d[i][k]+d[k+1][j], d[i][k] \\\\cdot d[k+1][j])] \\\\ (i \\\\lt j)[/math] Вычислим элементы таблицы[math]d[/math], тогда ответом на задачу будет значение[math]d[n][/math]. Рассмотрим ее, например, в такой формулировке. В арифметическом выражении, операндами которого являются целые числа, а операциями — бинарные арифметические'),\n",
       " Document(id='74de99e0-9371-4641-995a-f8d9e879dd3c', metadata={'title': 'Обратный оператор', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D1%8B%D0%B9_%D0%BE%D0%BF%D0%B5%D1%80%D0%B0%D1%82%D0%BE%D1%80'}, page_content='- Обратная матрица - Ядро и образ линейного оператора - Анин конспект'),\n",
       " Document(id='4629b2f9-0304-4ebb-a77d-2c6ad454cdd0', metadata={'title': 'Подсчёт числа остовных деревьев с помощью матрицы Кирхгофа', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%9F%D0%BE%D0%B4%D1%81%D1%87%D0%B5%D1%82_%D1%87%D0%B8%D1%81%D0%BB%D0%B0_%D0%BE%D1%81%D1%82%D0%BE%D0%B2%D0%BD%D1%8B%D1%85_%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D1%8C%D0%B5%D0%B2_%D1%81_%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D1%8B_%D0%9A%D0%B8%D1%80%D1%85%D0%B3%D0%BE%D1%84%D0%B0'}, page_content='СледствиеПри[math]s = t[/math]определитель произведения двух квадратных матриц порядка[math]s[/math]равен произведению определителей этих матриц - Связь матрицы Кирхгофа и матрицы инцидентности - Матрица Кирхгофа - Количество помеченных деревьев - Коды Прюфера - Асанов М., Баранский В., Расин В. - Дискретная математика: Графы, матроиды, алгоритмы — Ижевск: ННЦ \"Регулярная и хаотическая динамика\", 2001, 288 стр.'),\n",
       " Document(id='c9007047-0645-4c6a-b171-8221003c98de', metadata={'title': 'Многопоточность в машинном обучении', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D0%BF%D0%BE%D1%82%D0%BE%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%B2_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%BC_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B8'}, page_content='через матричные операции, которые легко параллелизируемы, например, можно обновлять вектор из оптимизируемых параметров через умножение на матрицы. При использованииметода наименьших квадратовпоиск коэффициентов регрессии сводится к нахождению псевдообратной матрицы. Хотя псевдообратную матрицу можно вычислить через обратную и для этого существуют параллельные алгоритмы, такой подход остается непрактичным. Более популярный способ, основанный на сингулярном разложении, можно сделать параллельным, если в процессе использовать метод Якоби для собственных значений и на каждом шаге обрабатывать несколько строк и столбцов. Также можно использовать параллельный алгоритм для QR-разложения как это'),\n",
       " Document(id='59cb337f-6c3f-4ef4-887d-1c9b70e96e7b', metadata={'title': 'Алгебра и геометрия 1 курс', 'url': 'https://neerc.ifmo.ru/wiki/index.php?title=%D0%90%D0%BB%D0%B3%D0%B5%D0%B1%D1%80%D0%B0_%D0%B8_%D0%B3%D0%B5%D0%BE%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D1%8F_1_%D0%BA%D1%83%D1%80%D1%81'}, page_content='Координация конспектов.Билеты второго семестра. - Линейные операторы и их матричная запись. Примеры - Пространство линейных операторов - Алгебра. Примеры. Изоморфизм алгебр. Алгебра операторов и матриц - Обратная матрица - Ядро и образ линейного оператора. Теорема о ядре и образе. Функции матриц и операторов. - Обратный оператор. Критерий существования обратного оператора. - Замена базиса. Преобразование координат векторов Х и Х* при замене базиса. Преобразование матрицы линейного оператора А при замене базиса. Преобразование подобия. - Тензоры (ковариантность, независимое от ПЛФ определение). Пространство тензоров. Свертка тензора. Транспонирование тензора. - Определитель линейного')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T11:27:07.242232Z",
     "start_time": "2025-05-25T11:27:07.237700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"\"\"You're a personal AI assistant.\n",
    "Don't tell the user about the context!\n",
    "Don't talk to the user about unnecessary topics other than the topic of context!\n",
    "Don't provide documents unnecessarily!\n",
    "Rely only on the suggested context and documents!\n",
    "You should use at least one provided document to explain!\n",
    "If the user asks about something else out of context, tell him \"Данный вопрос находится вне базы знаний\"\n",
    "Along with the response, send the sources from the metadata.\n",
    "\n",
    "Answer only on Russian, but you can think in whatever language you want.\n",
    "Present all mathematical formulas in LaTeX format.\n",
    "\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = G4FLLM(\n",
    "    provider=Provider.Qwen_Qwen_2_5,\n",
    "    model=\"qwen-qwen2-5\",\n",
    "    create_kwargs={\"temperature\": 0}\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever_from_llm, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "id": "7f95f4e5487fe894",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T11:28:06.108268Z",
     "start_time": "2025-05-25T11:27:11.171094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_prompt = \"\"\"Объясни архитектуру GPT. В чём отличие от RNN?\"\"\"\n",
    "print(\"AI: \", end=\"\")\n",
    "answer = \"\"\n",
    "for chunk in chain.stream(user_prompt):\n",
    "    answer += chunk\n",
    "    print(chunk, end=\"\")\n",
    "print()"
   ],
   "id": "7bbcbe53daff4d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: New g4f version: 0.5.3.1 (current: 0.5.2.8) | pip install -U g4f\n",
      "Архитектура GPT (Generative Pre-trained Transformer) основана на модели трансформера, которая была представлена в статье \"Attention is All You Need\" \\cite{attentionisallyouneed}. Трансформеры используют механизм внимания (attention mechanism), который позволяет модели эффективно обрабатывать последовательности данных, особенно длинные последовательности, без потери информации.\n",
      "\n",
      "### Архитектура GPT\n",
      "\n",
      "1. **Механизм Внимания (Attention Mechanism)**:\n",
      "   - В трансформере используется механизм внимания, который позволяет каждому элементу последовательности фокусироваться на других элементах. Это делается с помощью матриц \\( Q \\) (Query), \\( K \\) (Key) и \\( V \\) (Value).\n",
      "   - Формула для вычисления внимания:\n",
      "     \\[\n",
      "     \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
      "     \\]\n",
      "   - Здесь \\( d_k \\) — размерность ключей (keys).\n",
      "\n",
      "2. **Многослойный Декодер (Multi-Layer Decoder)**:\n",
      "   - GPT состоит из нескольких слоёв декодера, каждый из которых включает в себя подслой внимания и подслой полносвязной нейронной сети.\n",
      "   - Каждый слой имеет нормализацию (layer normalization) и остаточные соединения (residual connections).\n",
      "\n",
      "3. **Позиционные Эмбеддинги (Positional Embeddings)**:\n",
      "   - Поскольку трансформеры не учитывают порядок элементов в последовательности, используются позиционные эмбеддинги, которые добавляются к входным эмбеддингам слов.\n",
      "   - Позиционные эмбеддинги могут быть статическими (например, синусоидальные) или обучаемыми.\n",
      "\n",
      "4. **Маскирование (Masking)**:\n",
      "   - В декодере GPT используется маскирование, чтобы предотвратить доступ модели к будущим токенам при генерации текста. Это обеспечивает автогенерацию текста, где каждый следующий токен зависит только от предыдущих.\n",
      "\n",
      "### Отличие от RNN\n",
      "\n",
      "1. **Параллелизация**:\n",
      "   - **RNN**: Рекуррентные нейронные сети (RNN) обрабатывают данные последовательно, что ограничивает их параллельную обработку и увеличивает время обучения.\n",
      "   - **GPT**: Трансформеры могут обрабатывать все элементы последовательности одновременно благодаря механизму внимания, что значительно ускоряет обучение и инференс.\n",
      "\n",
      "2. **Длинные Зависимости**:\n",
      "   - **RNN**: RNN могут сталкиваться с проблемами долгосрочной зависимости (long-term dependencies), особенно если последовательность очень длинная.\n",
      "   - **GPT**: Механизм внимания в трансформерах позволяет модели эффективно улавливать зависимости между элементами, находящимися на большом расстоянии друг от друга.\n",
      "\n",
      "3. **Сложность и Параметры**:\n",
      "   - **RNN**: RNN имеют относительно простую архитектуру, но могут требовать большого количества параметров для достижения высокой производительности.\n",
      "   - **GPT**: Трансформеры могут иметь очень большое количество параметров (например, GPT-3 имеет 175 миллиардов параметров), что позволяет им достигать выдающихся результатов в различных задачах.\n",
      "\n",
      "4. **Устойчивость к Градиентам**:\n",
      "   - **RNN**: RNN часто сталкиваются с проблемами исчезающих и взрывных градиентов, что затрудняет обучение.\n",
      "   - **GPT**: Архитектура трансформера с остаточными соединениями и нормализацией помогает улучшить устойчивость к градиентам.\n",
      "\n",
      "Таким образом, GPT и трансформеры в целом предлагают более мощные и эффективные решения для задач обработки естественного языка по сравнению с RNN, особенно при работе с длинными последовательностями и большим объёмом данных.\n",
      "\n",
      "### Источники:\n",
      "- [Document(id='0f1dc374-931e-4158-829d-d52851acf182', metadata={'arxiv_url': 'https://arxiv.org/abs/2102.105351', 'id': '2102.10535v1'})]\n",
      "- [Document(id='241dc9d9-e697-48b9-8dff-cdfbbc5e7635', metadata={'arxiv_url': 'https://arxiv.org/abs/2210.070411', 'id': '2210.07041v1'})]\n",
      "- [Document(id='138a4d97-bb3a-4f99-a692-437cf4d59039', metadata={'arxiv_url': 'https://arxiv.org/abs/2202.033711', 'id': '2202.03371v1'})]\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
